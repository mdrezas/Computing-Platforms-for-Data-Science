{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DSE6000_Lab_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8OP7OLLj8py"
      },
      "source": [
        "#DSE6000 Lab-2 \n",
        "\n",
        "The purpose of this project is to leverage Spark DataFrame, SparkSQL, and Exploratory Data Analysis $(EDA)$ framework to:\n",
        "\n",
        "* Performs initial analysis to assess the quality of the data\n",
        "* Removes dirty data as and when discovered\n",
        "* Performs Exploratory Data Analysis (EDA) to determine the meaning of the data\n",
        "* Perform statistical analysis on COVID-19 Cases, Status, & Deaths\n",
        "\n",
        "Finally, compare Pandas and Pyspark Dataframe in terms of data processing, speed, flexibility, adaptability, advantages, and disadvantages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xx8rPfM-SnN"
      },
      "source": [
        "# Install Java, Spark, and Findspark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nK8l1E0-AgG"
      },
      "source": [
        "%%bash\n",
        "apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "[ ! -e \"$(basename spark-3.0.1-bin-hadoop2.7.tgz)\" ] && wget  http://apache.osuosl.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz  \n",
        "tar xf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnFgY_e9-dLm"
      },
      "source": [
        "# Set Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egw5Pb6E-Kth"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZbksICl-esc"
      },
      "source": [
        "# Start SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzzPgxFd-OeG"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# get a spark session. \n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7PKN0PY-r0_"
      },
      "source": [
        "# Create the Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XB5j0M8z-qqc",
        "outputId": "5b777f0c-eac2-4a8c-849f-8e50bfb618c4"
      },
      "source": [
        "! [ ! -e \"$(basename detroit_demolitions_dataset.csv)\" ] && wget  https://storage.googleapis.com/files.mobibootcamp.com/2020-datafiles/Covid-19_Tests_by_County_2020-09-17_702630_7.csv\n",
        "df = spark.read.csv('Covid-19_Tests_by_County_2020-09-17_702630_7.csv',\n",
        "                      header= True, \n",
        "                      inferSchema = True)\n",
        "\n",
        "\n",
        "print(df.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-26 02:09:33--  https://storage.googleapis.com/files.mobibootcamp.com/2020-datafiles/Covid-19_Tests_by_County_2020-09-17_702630_7.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.26.128, 172.217.193.128, 172.217.204.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.26.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12780 (12K) [application/octet-stream]\n",
            "Saving to: ‘Covid-19_Tests_by_County_2020-09-17_702630_7.csv’\n",
            "\n",
            "\r          Covid-19_   0%[                    ]       0  --.-KB/s               \rCovid-19_Tests_by_C 100%[===================>]  12.48K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-26 02:09:33 (46.4 MB/s) - ‘Covid-19_Tests_by_County_2020-09-17_702630_7.csv’ saved [12780/12780]\n",
            "\n",
            "['COUNTY', 'TestType', 'Count', 'RatePerMillion', 'Updated']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfuhuLC3_I-R"
      },
      "source": [
        "## Data Exploration\n",
        "\n",
        "Performs initial analysis to assess the quality of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LdB4Syd4knI"
      },
      "source": [
        "Count number of rows and columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpLECk4gcS-L",
        "outputId": "ed7d6b8d-dc6f-48b7-e1e0-a24a316ae4aa"
      },
      "source": [
        "print((df.count(), len(df.columns)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(258, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEIzuOOs4vN4"
      },
      "source": [
        "Print the Data Types"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcotvDNgcgYa",
        "outputId": "58c704a6-1071-4fb0-ba2d-127b94a51a1c"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('COUNTY', 'string'),\n",
              " ('TestType', 'string'),\n",
              " ('Count', 'int'),\n",
              " ('RatePerMillion', 'int'),\n",
              " ('Updated', 'string')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdLPld4a4yl2"
      },
      "source": [
        "Print the $Shcema$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXgNxNuQfYV3",
        "outputId": "b899c775-9e1a-424e-e87e-3c1b6d30a377"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- COUNTY: string (nullable = true)\n",
            " |-- TestType: string (nullable = true)\n",
            " |-- Count: integer (nullable = true)\n",
            " |-- RatePerMillion: integer (nullable = true)\n",
            " |-- Updated: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMsC613zcms-"
      },
      "source": [
        "Check columns with $NaN$ values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmSGbFrQcnfx",
        "outputId": "8fa00c2f-c6d0-44a9-edf5-d6d5566b95e1"
      },
      "source": [
        "from pyspark.sql.functions import isnan, when, count, col\n",
        "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+--------+-----+--------------+-------+\n",
            "|COUNTY|TestType|Count|RatePerMillion|Updated|\n",
            "+------+--------+-----+--------------+-------+\n",
            "|     0|       0|    0|             6|      0|\n",
            "+------+--------+-----+--------------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXVAKKVR3sPe"
      },
      "source": [
        "Print the rows with $NULL$ values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXO_E3LTG-YZ",
        "outputId": "4e9e0473-20a5-4fd0-8388-1fa6bd1bed77"
      },
      "source": [
        "df.registerTempTable(\"mytable\") \n",
        "df_Null = spark.sql(\"\"\"SELECT * FROM mytable where RatePerMillion IS NULL\"\"\")\n",
        "print(\"Rows with null values\")\n",
        "df_Null.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rows with null values\n",
            "+------------+----------+------+--------------+-------------------+\n",
            "|      COUNTY|  TestType| Count|RatePerMillion|            Updated|\n",
            "+------------+----------+------+--------------+-------------------+\n",
            "|Correctional|  Serology| 10990|          null|2020/09/17 13:31:36|\n",
            "|Correctional|Diagnostic| 83705|          null|2020/09/17 13:31:36|\n",
            "|Correctional|     Total| 94695|          null|2020/09/17 13:31:36|\n",
            "|     Unknown|  Serology| 19839|          null|2020/09/17 13:31:36|\n",
            "|     Unknown|Diagnostic|223551|          null|2020/09/17 13:31:36|\n",
            "|     Unknown|     Total|243390|          null|2020/09/17 13:31:36|\n",
            "+------------+----------+------+--------------+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVryfSdQcyuX"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVc94Vx3dg4G"
      },
      "source": [
        "Drop the $Updated$ column with the static timestamp, because it will not add any values to the analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "703rIFU2dFSH",
        "outputId": "c68afb50-a46d-44b4-ea39-c26fa2847f4d"
      },
      "source": [
        "df.select('Updated').distinct().rdd.map(lambda r: r[0]).collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2020/09/17 13:31:36']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euU8_Moddk0r",
        "outputId": "49f3f0bb-c77c-4182-b9ad-59db99e60095"
      },
      "source": [
        "column_to_drop = ['Updated']\n",
        "df = df.drop(*column_to_drop)\n",
        "df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----------+-----+--------------+\n",
            "|COUNTY|  TestType|Count|RatePerMillion|\n",
            "+------+----------+-----+--------------+\n",
            "|Alcona|Diagnostic| 1974|        189716|\n",
            "|Alcona|  Serology|   51|          4901|\n",
            "|Alcona|     Total| 2025|        194618|\n",
            "+------+----------+-----+--------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytlOX5tTe56o"
      },
      "source": [
        "##Handling $missing$ values. \n",
        "\n",
        "Since the reasons for the $RatePerMillion$ column's missing values are not specified, and Michigan does not have any $County$ with such a name. Therefore, it is safe to drop these six rows with NaN values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fyeCTenia5T"
      },
      "source": [
        "df = df.filter(df.RatePerMillion. isNotNull())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5YX61E-gfV-"
      },
      "source": [
        "Number of rows after dropping the NULL values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6butIzdygkRo",
        "outputId": "c303d8ee-28ed-4c6a-a097-b4e05ff8f811"
      },
      "source": [
        "print(\"Number of rows & columns after dropping NaN values from the RatePerMillion column:\")\n",
        "print((df.count(), len(df.columns)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows & columns after dropping NaN values from the RatePerMillion column:\n",
            "(252, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JhgNTl3jFuv"
      },
      "source": [
        "# Data Transformation\n",
        "\n",
        "\n",
        "Rename column's name to $camelcase$ for consistency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnQwCIp0_NMU",
        "outputId": "41d4646c-774f-434c-91ab-a2d2b0fb842b"
      },
      "source": [
        "df = df.toDF('County', 'TestType', 'Count', 'RatePerMillion')\n",
        "df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----------+-----+--------------+\n",
            "|County|  TestType|Count|RatePerMillion|\n",
            "+------+----------+-----+--------------+\n",
            "|Alcona|Diagnostic| 1974|        189716|\n",
            "|Alcona|  Serology|   51|          4901|\n",
            "|Alcona|     Total| 2025|        194618|\n",
            "+------+----------+-----+--------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IToRFLezafJ"
      },
      "source": [
        "Format & replace some of the county's name in the County column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoR41B-dwTyz"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "df = df.withColumn('County', regexp_replace('County', 'St', 'Saint'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAeVlEBNxBs-",
        "outputId": "ebc64270-03fc-4032-d03f-cb8223948009"
      },
      "source": [
        "from pyspark.sql.functions import col\n",
        "df.where(col(\"County\").isin({\"Saint Joseph\", \"Saint Clair\"})).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+----------+-----+--------------+\n",
            "|      County|  TestType|Count|RatePerMillion|\n",
            "+------------+----------+-----+--------------+\n",
            "| Saint Clair|  Serology| 4024|         25288|\n",
            "| Saint Clair|Diagnostic|31351|        197017|\n",
            "| Saint Clair|     Total|35375|        222305|\n",
            "|Saint Joseph|  Serology|  594|          9743|\n",
            "|Saint Joseph|Diagnostic|16586|        272062|\n",
            "|Saint Joseph|     Total|17180|        281806|\n",
            "+------------+----------+-----+--------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhwPNRS6z0WC"
      },
      "source": [
        "Add a new column $State$ with static value = $MI$ to use it with the county's FIPS code for visualizing data with the map  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEkRHKxMz4Co",
        "outputId": "ad323a09-c242-4f84-879a-2a93a7a4ee30"
      },
      "source": [
        "from pyspark.sql.functions import lit\n",
        "df = df.withColumn(\"State\", lit('MI'))\n",
        "df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----------+-----+--------------+-----+\n",
            "|County|  TestType|Count|RatePerMillion|State|\n",
            "+------+----------+-----+--------------+-----+\n",
            "|Alcona|Diagnostic| 1974|        189716|   MI|\n",
            "|Alcona|  Serology|   51|          4901|   MI|\n",
            "|Alcona|     Total| 2025|        194618|   MI|\n",
            "+------+----------+-----+--------------+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7Qul3W57Z3t"
      },
      "source": [
        "Create a data frame with a subset of top 10 $Diagnostic$ counts and sort them in ascending order ($Usage:$ To visualize top 10 counties with Diagnostic counts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmcVL4GD3BIK",
        "outputId": "2893b546-b296-4366-a96d-65f3846a792c"
      },
      "source": [
        "top_ten_Diagnostic = df.where((col(\"Count\") > 74000) & (col(\"TestType\") == \"Diagnostic\"))\n",
        "top_ten_Diagnostic= top_ten_Diagnostic.orderBy(top_ten_Diagnostic.Count.desc())\n",
        "top_ten_Diagnostic.show(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+----------+------+--------------+-----+\n",
            "|      County|  TestType| Count|RatePerMillion|State|\n",
            "+------------+----------+------+--------------+-----+\n",
            "|       Wayne|Diagnostic|375403|        340097|   MI|\n",
            "|     Oakland|Diagnostic|351635|        279612|   MI|\n",
            "|Detroit City|Diagnostic|259362|        399139|   MI|\n",
            "|        Kent|Diagnostic|231809|        352854|   MI|\n",
            "|      Macomb|Diagnostic|222400|        254470|   MI|\n",
            "|   Washtenaw|Diagnostic|128943|        350769|   MI|\n",
            "|      Ingham|Diagnostic|100272|        342920|   MI|\n",
            "|      Ottawa|Diagnostic| 94172|        322695|   MI|\n",
            "|     Genesee|Diagnostic| 89810|        221309|   MI|\n",
            "|   Kalamazoo|Diagnostic| 74258|        280149|   MI|\n",
            "+------------+----------+------+--------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB5QEsyB7ruv"
      },
      "source": [
        "##Create & Transform $Supplimentary$ Dataframe "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLCH7n0Z3cjV",
        "outputId": "e29de6d3-8ef3-4717-c141-2d712dec9a0b"
      },
      "source": [
        "! [ ! -e \"$(basename detroit_demolitions_dataset.csv)\" ] && wget  https://storage.googleapis.com/files.mobibootcamp.com/2020-datafiles/Cases_and_Deaths_by_County_2020-09-17_702626_7.csv\n",
        "df_suppliment_data = spark.read.csv('Cases_and_Deaths_by_County_2020-09-17_702626_7.csv',\n",
        "                      header= True, \n",
        "                      inferSchema = True)\n",
        "column_to_drop = ['Updated']\n",
        "df_suppliment_data = df_suppliment_data.drop(*column_to_drop)\n",
        "df_suppliment_data = df_suppliment_data.toDF('County', 'CaseStatus', 'Cases', 'Deaths')\n",
        "df_suppliment_data.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-26 02:09:53--  https://storage.googleapis.com/files.mobibootcamp.com/2020-datafiles/Cases_and_Deaths_by_County_2020-09-17_702626_7.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.204.128, 172.217.203.128, 172.253.123.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7840 (7.7K) [application/octet-stream]\n",
            "Saving to: ‘Cases_and_Deaths_by_County_2020-09-17_702626_7.csv’\n",
            "\n",
            "\r          Cases_and   0%[                    ]       0  --.-KB/s               \rCases_and_Deaths_by 100%[===================>]   7.66K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-26 02:09:53 (81.2 MB/s) - ‘Cases_and_Deaths_by_County_2020-09-17_702626_7.csv’ saved [7840/7840]\n",
            "\n",
            "root\n",
            " |-- County: string (nullable = true)\n",
            " |-- CaseStatus: string (nullable = true)\n",
            " |-- Cases: integer (nullable = true)\n",
            " |-- Deaths: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3Hi5-cOyVAq"
      },
      "source": [
        "Use $where$ & $order$ by clause to filter and join actual Dataframe with the supplementary Dataframe by $County$ column.\n",
        "\n",
        "(Usage: To visualize the correlation between numerical columns using the Heat Map & Linear Regression Model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQs6Scdog7y7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef7dfae-abcd-4525-eb3d-c01333ce34ae"
      },
      "source": [
        "df.registerTempTable(\"DiagnosticTable\")\n",
        "df_suppliment_data.registerTempTable(\"CaseStatusTable\")\n",
        "df_Join =   spark.sql(\"\"\"SELECT d.County, \n",
        "    d.TestType, \n",
        "    d.Count, \n",
        "    d.RatePerMillion, \n",
        "    c.CaseStatus,\n",
        "    c.Cases,\n",
        "    c.Deaths \n",
        "    from DiagnosticTable d LEFT JOIN CaseStatusTable c \n",
        "    ON d.County = c.County\n",
        "    where d.TestType = \"Diagnostic\" and c.CaseStatus = \"Confirmed\" and c.Deaths != 0\n",
        "    order by Count desc\n",
        "\"\"\")\n",
        "df_Join.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+----------+------+--------------+----------+-----+------+\n",
            "|      County|  TestType| Count|RatePerMillion|CaseStatus|Cases|Deaths|\n",
            "+------------+----------+------+--------------+----------+-----+------+\n",
            "|       Wayne|Diagnostic|375403|        340097| Confirmed|17568|  1265|\n",
            "|     Oakland|Diagnostic|351635|        279612| Confirmed|15863|  1141|\n",
            "|Detroit City|Diagnostic|259362|        399139| Confirmed|14131|  1517|\n",
            "|        Kent|Diagnostic|231809|        352854| Confirmed| 8472|   170|\n",
            "|      Macomb|Diagnostic|222400|        254470| Confirmed|13368|   959|\n",
            "+------------+----------+------+--------------+----------+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfOb3sLZOXkB"
      },
      "source": [
        "# Statistics on COVID-19 Cases, Status, & Deaths Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLU-VshGUoTs"
      },
      "source": [
        "Top 10 Counties with Confirm Cases & Deaths Counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECLhwExbNOvJ",
        "outputId": "4dd8bb2f-30d6-44d3-c4d2-3646b2931163"
      },
      "source": [
        "df_Join.registerTempTable(\"df_Join\")\n",
        "top_counties = spark.sql(\"\"\"\n",
        "    SELECT County, Cases, Deaths  \n",
        "\tFROM df_Join \n",
        "  where County != \"Detroit City\"\n",
        "  order by Deaths desc limit 10\n",
        "\"\"\")\n",
        "top_counties.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-----+------+\n",
            "|   County|Cases|Deaths|\n",
            "+---------+-----+------+\n",
            "|    Wayne|17568|  1265|\n",
            "|  Oakland|15863|  1141|\n",
            "|   Macomb|13368|   959|\n",
            "|  Genesee| 3441|   281|\n",
            "|     Kent| 8472|   170|\n",
            "|  Saginaw| 2653|   134|\n",
            "|Washtenaw| 2916|   115|\n",
            "|Kalamazoo| 2108|    89|\n",
            "|  Berrien| 1583|    72|\n",
            "| Muskegon| 1374|    69|\n",
            "+---------+-----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlmd70rJU0i9"
      },
      "source": [
        "Using mean function to calculate mean death rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xpU_mIKiERm",
        "outputId": "a8d86494-0708-4838-ce1f-fb9d64730a29"
      },
      "source": [
        "from pyspark.sql.functions import mean\n",
        "df_suppliment_data_filter = df_suppliment_data.where(col(\"Deaths\") > 0)\n",
        "df_suppliment_data_filter.select(mean(\"Deaths\").alias(\"Mean Death Rate\")).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+\n",
            "|  Mean Death Rate|\n",
            "+-----------------+\n",
            "|73.21052631578948|\n",
            "+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyZHkTzb7lzF"
      },
      "source": [
        "Using $Sum()$ and $partitionBy()$ function to calculate the Deaths Percentage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNZAFYqgwNLX",
        "outputId": "d8660f39-dc5f-4e8a-b304-790dddcc779c"
      },
      "source": [
        "import pyspark.sql.functions as f\n",
        "from pyspark.sql.window import Window\n",
        "df_percent = df_Join.withColumn('DeathsPercent', f.col('Deaths')/f.sum('Deaths').over(Window.partitionBy()))\n",
        "col_to_drop = ['TestType', 'RatePerMillion', 'Count', 'CaseStatus']\n",
        "df_percent = df_percent.drop(*col_to_drop)\n",
        "df_percent.orderBy('Deaths','DeathsPercent', ascending=False).show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----+------+--------------------+\n",
            "|      County|Cases|Deaths|       DeathsPercent|\n",
            "+------------+-----+------+--------------------+\n",
            "|Detroit City|14131|  1517| 0.23367221195317314|\n",
            "|       Wayne|17568|  1265| 0.19485520640788662|\n",
            "|     Oakland|15863|  1141| 0.17575477510782503|\n",
            "|      Macomb|13368|   959| 0.14772027110289587|\n",
            "|     Genesee| 3441|   281| 0.04328404189772027|\n",
            "|        Kent| 8472|   170| 0.02618607516943931|\n",
            "|     Saginaw| 2653|   134|0.020640788662969808|\n",
            "|   Washtenaw| 2916|   115| 0.01771410967344424|\n",
            "|   Kalamazoo| 2108|    89|0.013709180529882932|\n",
            "|     Berrien| 1583|    72|0.011090573012939002|\n",
            "+------------+-----+------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpfGnTSW2bE1"
      },
      "source": [
        "Find the $Max$ and $Min$ count of confirmed cases\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRNFfpzBif68",
        "outputId": "96ea34cc-4957-4580-fa21-1e78cbf9ff99"
      },
      "source": [
        "from pyspark.sql.functions import max,min\n",
        "df_suppliment_data_filter.select(max(\"Cases\").alias(\"MaxCasesCount\"), min(\"Cases\").alias(\"MinCasesCount\")).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-------------+\n",
            "|MaxCasesCount|MinCasesCount|\n",
            "+-------------+-------------+\n",
            "|        17568|            4|\n",
            "+-------------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy-U_2S12sOU"
      },
      "source": [
        "Group by aggregated sum for $TestType$ categorical column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW2bCHVGjizq",
        "outputId": "3c331b19-8b45-4131-cf32-e0f2f4b2cd19"
      },
      "source": [
        "df.groupBy(\"TestType\").agg(sum(\"Count\").alias(\"TestTypeTotal\")).show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-------------+\n",
            "|  TestType|TestTypeTotal|\n",
            "+----------+-------------+\n",
            "|     Total|      3228116|\n",
            "|  Serology|       249151|\n",
            "|Diagnostic|      2978965|\n",
            "+----------+-------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKlruRHuxoBC"
      },
      "source": [
        "#Advantages and Disadvantages of PySpark and Pandas: \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPJJe9mV_2-8"
      },
      "source": [
        "Pandas data frames are in-memory, single-server, High-performance, easy-to-use data structures, which also limits its processing power to a single server. While, PySpark is a Python API for Spark, distributed on the spark clusters, leveraged for big data processing.\n",
        "##Advantages/Disadvantages of Pandas & Pyspark\n",
        "* Pandas DataFrames processing is faster, while PySpark DataFrames are lazy.\n",
        "* PySpark DataFrame runs parallel on different nodes in cluster but, pandas DataFrames are in-memory, single-server based.\n",
        "* PySpark DataFrames are immutable, but in pandas, this is not the case.\n",
        "* Pandas API supports more operations than PySpark DataFrame.\n",
        "* Pandas are more flexible to perform complex tasks compare to PySpark DataFrame.\n",
        "\n",
        "Finally, Pandas have powerful and flexible data analysis/manipulation libraries for Python. While PySpark is a Python API for Spark that harnesses the power of Python and Apache Spark to process Big Data, but to date Pandas API is more powerful than Pyspark."
      ]
    }
  ]
}